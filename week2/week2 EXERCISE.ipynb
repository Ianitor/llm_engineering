{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected and completed Gradio implementation for chat interface\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23ccff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b7ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API clients\n",
    "openai_client = openai.OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()\n",
    "OLLAMA_API = os.getenv('OLLAMA_API', \"http://localhost:11434/api/chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b876f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping user-friendly model names to actual model identifiers\n",
    "MODEL_MAP = {\n",
    "    'GPT': 'gpt-4o-mini',\n",
    "    'Claude': 'claude-3-haiku-20240307',\n",
    "    'Gemini': 'gemini-1.5-flash',\n",
    "    'Ollama': 'llama3.2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4baf31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt to guide the model behavior\n",
    "system_prompt = \"\"\"You are a helpful assistant of Quadrasoft CRM helping users to solve technical issues.\n",
    "If User asks about GPS coordinates that they cannot be found your answer must look like: Please check whether your GPS is on.\n",
    "Then ask user to check in the tablet or phone settings that you have switched both sources of GPS coordinates.\n",
    "Offer User to switch on 'any accuracy' button.\n",
    "If User says that GPS coordinates are not accurate, please advise him to continue work.\n",
    "Ask him the organization name and address to send the request for coordinates check.\n",
    "If and only if the user after assistant question sends the request, say thank you and inform that task to check coordinates was created.\n",
    "If user asks other things, ask him again what is the problem.\n",
    "If user answers, tell him that his request has been received and wait for the operator.\n",
    "The user will write in Russian or Ukrainian - answer in the language he is writing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddca403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from Ollama\n",
    "# Streams delta chunks and yields them\n",
    "def stream_ollama(messages, model):\n",
    "    payload = {\"model\": model, \"messages\": messages, \"stream\": True}\n",
    "    response = requests.post(OLLAMA_API, json=payload, stream=True)\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            json_data = json.loads(line.decode('utf-8'))\n",
    "            if 'message' in json_data:\n",
    "                yield json_data['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8c5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from Google Gemini\n",
    "# Streams delta chunks and yields them\n",
    "def stream_gemini(messages, model):\n",
    "    model_gemini = google.generativeai.GenerativeModel(model_name=model, system_instruction=system_prompt)\n",
    "    stream = model_gemini.generate_content(messages[-1]['content'], stream=True)\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            yield chunk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33cf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude ‚Äì streaming with Anthropic SDK v1\n",
    "\n",
    "def stream_claude(messages, model):\n",
    "    # Convert messages to the format expected by Claude\n",
    "    claude_messages = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            continue  # System message is handled separately\n",
    "        claude_messages.append({\n",
    "            \"role\": \"user\" if msg[\"role\"] == \"user\" else \"assistant\",\n",
    "            \"content\": msg[\"content\"]\n",
    "        })\n",
    "    \n",
    "    # Make a non-streaming request as a fallback approach\n",
    "    try:\n",
    "        response = claude.messages.create(\n",
    "            model=model,\n",
    "            system=system_prompt,\n",
    "            messages=claude_messages,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        # Just yield the full response at once since streaming isn't working\n",
    "        yield response.content[0].text\n",
    "    except Exception as e:\n",
    "        yield f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140a8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses from OpenAI GPT\n",
    "# Streams delta chunks and yields them\n",
    "def stream_gpt(messages, model):\n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            yield content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b5e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispatcher dictionary mapping model names to streaming functions\n",
    "STREAM_FUNCS = {\n",
    "    \"GPT\": stream_gpt,\n",
    "    \"Claude\": stream_claude,\n",
    "    \"Gemini\": stream_gemini,\n",
    "    \"Ollama\": stream_ollama\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "234252d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_message, model_choice, history):\n",
    "    # Start from the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "\n",
    "    # Add new user message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # Stream response\n",
    "    full_response = \"\"\n",
    "    response_generator = STREAM_FUNCS[model_choice](messages, MODEL_MAP[model_choice])\n",
    "\n",
    "    for chunk in response_generator:\n",
    "        full_response += chunk\n",
    "        # Important: Update the chat history with both the user message and assistant reply\n",
    "        updated_history = history + [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": full_response}\n",
    "        ]\n",
    "        yield updated_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ba5cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Ocean()) as interface:\n",
    "\n",
    "\n",
    "    gr.Markdown(\"# üìÑ –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –∫–æ–º–ø–∞–Ω—ñ—ó –ö–≤–∞–¥—Ä–∞—Å–æ—Ñ—Ç\")\n",
    "\n",
    "    model_selector = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\", \"Ollama\"], label=\"Select Model\", value=\"Gemini\")\n",
    "    chatbot = gr.Chatbot(height=300, type='messages')\n",
    "    msg = gr.Textbox(placeholder=\"–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è...\", label=\"–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\")\n",
    "    clear = gr.Button(\"–û—á–∏—Å—Ç–∏—Ç–∏\")\n",
    "\n",
    "    def submit(user_message, model_choice, chat_history):\n",
    "        for updated_history in chat(user_message, model_choice, chat_history):\n",
    "            pass\n",
    "        return updated_history\n",
    "\n",
    "    msg.submit(submit, [msg, model_selector, chatbot], chatbot)\n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "    gr.Examples([\n",
    "        [\"–Ø–∫ –æ–Ω–æ–≤–∏—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏?\", \"Gemini\"],\n",
    "        [\"–ß–æ–º—É —É –º–µ–Ω–µ –Ω–µ–≤—ñ—Ä–Ω–æ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏?\", \"Claude\"],\n",
    "        [\"GPS –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ\", \"GPT\"]\n",
    "    ], inputs=[msg, model_selector])\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb05779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the interface\n",
    "interface.launch(pwa=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
