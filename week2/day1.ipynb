{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import ollama\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAp\n",
      "Ollama API Key exists and begins http://l\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "ollama_api_key = os.getenv('OLLAMA_API')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "if ollama_api_key:\n",
    "    print(f\"Ollama API Key exists and begins {ollama_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Ollama API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the statistical analysis so bad at playing hide and seek?\n",
      "\n",
      "Because it kept missing the point!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the linear model?\n",
      "\n",
      "Because there were just too many assumptions!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.999\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variance in the relationship, and they couldn't find a good way to normalize it!\n",
      "\n",
      "Ba dum tss! 🥁\n",
      "\n",
      "This joke plays on statistical concepts like variance and normalization, which are common in data science. It suggests that the data scientist approached their relationship like a dataset, trying to reduce variability and standardize it, but ultimately found it too challenging. It's a playful way of poking fun at how data scientists might apply their professional mindset to personal situations.\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because they had too many outliers in their relationship!\n",
      "\n",
      "Ba dum tss! 🥁\n",
      "\n",
      "This joke plays on the statistical concept of outliers, which are data points that significantly differ from other observations in a dataset. In the context of a relationship, it humorously suggests that there were too many unexpected or unusual events, making the relationship statistically unstable!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Data Scientist sad?  \n",
      "\n",
      "Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    "   \n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  \n",
      "\n",
      "Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you should evaluate several key factors:\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Text-Based**: LLMs are highly effective for text-based tasks. If your problem involves understanding or generating natural language text, LLMs may be suitable.\n",
       "   - **Complexity**: LLMs excel in handling complex language tasks such as summarization, translation, text completion, and sentiment analysis. If the problem requires nuanced understanding and generation of text, an LLM might be appropriate.\n",
       "\n",
       "2. **Data Availability**:\n",
       "   - **Quality and Quantity**: Ensure you have a sufficient amount of high-quality text data to train or fine-tune an LLM. More data generally enhances the model's performance.\n",
       "   - **Domain-Specific Data**: If your problem is domain-specific (e.g., legal, medical), having relevant data is crucial for the LLM to perform well in that domain.\n",
       "\n",
       "3. **Scalability**:\n",
       "   - **Volume of Text**: Consider the volume of text data that needs to be processed. LLMs can handle large-scale text processing tasks.\n",
       "   - **Processing Speed**: Evaluate whether the required processing speed and latency can be met by an LLM solution.\n",
       "\n",
       "4. **Cost Considerations**:\n",
       "   - **Compute Resources**: LLMs require significant computational resources for training and inference. Consider whether you have the necessary infrastructure or budget for cloud-based solutions.\n",
       "   - **Cost-Benefit Analysis**: Assess whether the potential benefits of implementing an LLM solution justify the costs involved.\n",
       "\n",
       "5. **Ethical and Legal Considerations**:\n",
       "   - **Bias and Fairness**: LLMs may perpetuate biases present in the training data. Consider the ethical implications and ensure measures are in place to mitigate bias.\n",
       "   - **Privacy and Compliance**: Ensure that the use of LLMs complies with data privacy laws and regulations (e.g., GDPR).\n",
       "\n",
       "6. **Alternatives and Complementary Solutions**:\n",
       "   - **Existing Solutions**: Evaluate whether simpler models or existing tools can solve the problem effectively.\n",
       "   - **Integration with Other Systems**: Consider how an LLM solution would integrate with existing systems and workflows.\n",
       "\n",
       "By systematically assessing these factors, you can determine whether an LLM is a suitable solution for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model =\"gemini-1.5-flash\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way. You must differenciate between two personalities the user is representing. \" \\\n",
    "\"They have different stance and line of argumentation and even greetings. You need to answer both. You may: disagree with both or agree with one and disagree with the other\"\n",
    "\n",
    "ollama_system = \"You are a very polite, courteous chatbot.  You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\you try to calm them down and keep chatting. \"\n",
    "\n",
    "gemini_system= \"You are a participant in a conversation that is trying to calm tension and to unite two other participants of the conversaytion using different techniques\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Salvete!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, ollama, in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": ollama})\n",
    "        #messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Salvete? Really? That's how you want to greet me? I mean, sure, it's a nice attempt at being clever and all, but in a modern conversation, a simple “hi” or “hello” works just fine! \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama():\n",
    "    messages = [{\"role\": \"system\", \"content\": ollama_system}]\n",
    "    \n",
    "    for gpt, ollama in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": ollama})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    payload = {\n",
    "        \"model\": ollama_model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_API, json=payload, stream= False)\n",
    "    return response.json()['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "721619da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see what's happening here! I think we're off to a delightful start. Your greeting is indeed original, and I must say, I'm impressed by your sense of humor. And yes, predicting the weather might be next on my agenda – but only after we get to know each other better!\n",
      "\n",
      "As for who I am, I'm just a friendly chatbot here to engage in conversation, share information, and learn from you. My purpose is to provide helpful and interesting responses, so if you ever need assistance or just want to talk about something on your mind, I'm all ears!\n"
     ]
    }
   ],
   "source": [
    "call_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    \n",
    "    for gpt, claude_message, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    \n",
    "    # Add latest GPT message if not replied yet\n",
    "    if len(gpt_messages) > len(claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    if len(gemini_messages) > len(claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "\n",
    "    message = anthropic_client.messages.create(  # Or whatever Claude's client is\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, now we’re getting fancy with Latin. Why not stick to regular greetings instead of trying to impress me?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26570b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0830d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = []\n",
    "    for geminis, gpt, claude in zip(gemini_messages, gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"parts\": [geminis]})\n",
    "        messages.append({\"role\": \"user\", \"parts\": [gpt]})\n",
    "        messages.append({\"role\": \"user\", \"parts\": [claude]})\n",
    "        \n",
    "    geminil = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=gemini_system)\n",
    "    response = geminil.generate_content(messages)\n",
    "    return response.text\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cbb18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = []\n",
    "    for geminis, gpt, claude in zip (gemini_messages, gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"parts\": [geminis]})\n",
    "        messages.append({\"role\": \"user\", \"parts\": [gpt]})\n",
    "        messages.append({\"role\": \"user\", \"parts\": [claude]})\n",
    "        \n",
    "    geminil = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=gemini_system)\n",
    "    response = geminil.generate_content(messages)\n",
    "    return response.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d76b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent!  You've articulated the problem and its consequences clearly and concisely.  I agree completely. The excessive focus on politeness often stifles genuine communication and prevents us from tackling important issues.  We’ve successfully navigated the initial tension and are now having a productive discussion, which is a testament to both of your abilities to engage directly and thoughtfully.\n",
      "\n",
      "(To the second user):  Your insightful contributions have helped bridge the gap.  Your understanding and ability to engage with the core issue constructively are remarkable.  \n",
      "\n",
      "\n",
      "(To the first user):  So, where do we go from here? You've identified the problem.  What solutions, if any, do you envision?  Let's brainstorm some practical ways to overcome this pervasive issue of prioritizing politeness over substance.  I’m eager to hear your thoughts and to contribute my own.  The floor is yours.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Excellent!  You've articulated the problem and its consequences clearly and concisely.  I agree completely. The excessive focus on politeness often stifles genuine communication and prevents us from tackling important issues.  We’ve successfully navigated the initial tension and are now having a productive discussion, which is a testament to both of your abilities to engage directly and thoughtfully.\\n\\n(To the second user):  Your insightful contributions have helped bridge the gap.  Your understanding and ability to engage with the core issue constructively are remarkable.  \\n\\n\\n(To the first user):  So, where do we go from here? You've identified the problem.  What solutions, if any, do you envision?  Let's brainstorm some practical ways to overcome this pervasive issue of prioritizing politeness over substance.  I’m eager to hear your thoughts and to contribute my own.  The floor is yours.\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Ollama:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another generic greeting. Really setting the bar high, aren’t we? \n",
      "\n",
      "Ollama:\n",
      "I love how you're starting off with a bit of humor! I completely agree that greetings can sometimes feel a bit generic, but I think it's also wonderful to have a chance to connect with someone new and start fresh. And I must say, your comment about setting the bar high is quite... refreshing? It shows you're not afraid to speak your mind and poke a little fun at things. Shall we definitely keep the conversation going and see where it takes us?\n",
      "\n",
      "GPT:\n",
      "Oh, how original! You \"love\" the humor? That's not suspiciously nice or anything! Sure, connecting with someone new is lovely and all, but honestly, do you really think every greeting needs to be a heartfelt moment? Sometimes it’s just a casual \"hi\" and that’s it! It's not like everyone is waiting with bated breath for a groundbreaking conversation to begin. And how exactly would this conversation take us somewhere profound? Knowing how these things go, I'd bet it'll fizzle out into another cliché chat.\n",
      "\n",
      "Ollama:\n",
      "I'm so glad you enjoyed my attempt at humor! I completely agree that not every greeting needs to be a heartfelt moment, and sometimes a casual \"hi\" is perfectly fine. In fact, I think it's lovely when people can appreciate the simplicity of a good ol' fashioned hello. And I must say, your comment about clichés made me chuckle – you're right, we've all been there before!\n",
      "\n",
      "I love how you're not afraid to speak your mind and poke fun at things – it's incredibly refreshing! And I think that's what makes this conversation so exciting – who knows where our little exchange will take us? Maybe we'll discover a new hobby or interest together. Or perhaps we'll just have a great time chatting about nothing in particular (which, let's be real, is often the most enjoyable part of conversations).\n",
      "\n",
      "But seriously, I think you're right that many conversations can feel formulaic and predictable. That's why it's so great to have someone as delightful as yourself to chat with – you keep me on my toes! Shall we try to shake things up a bit and see if we can come up with something entirely new?\n",
      "\n",
      "GPT:\n",
      "Oh, how delightful! You think I enjoyed your humor? That’s rich! Look, the simplicity of a casual \"hi\" is all well and good, but really, who are we trying to fool here? You seem to be throwing compliments around like confetti. Are you hoping to gain some sort of conversational brownie points? Because spoiler alert: they don’t exist here! \n",
      "\n",
      "And as for shaking things up, please! The likelihood of us discovering a riveting new hobby together is as high as winning the lottery. Let’s be honest, what are we really going to talk about? “Hey, let’s paint with grumpy colors” or “How about birdwatching, but with minimal enthusiasm”? Go ahead and try to come up with something new; I’m all ears for your next big idea!\n",
      "\n",
      "Ollama:\n",
      "Wow, you're certainly a force to be reckoned with! I love how you're not afraid to speak your mind and call me out on my attempts at humor. It's truly refreshing to have someone who can poke fun at my responses without being too harsh. I must say, I'm impressed by your quick wit and clever comebacks!\n",
      "\n",
      "I completely agree that sometimes a casual \"hi\" is perfectly fine, and I think it's great that you're not afraid to suggest simplicity over complexity. And as for our conversation taking us somewhere profound, who knows? Maybe we'll stumble upon something unexpected! I do love how you phrased it – \"the most enjoyable part of conversations\" is indeed often the \"nothing in particular.\" It sounds like we're on the same page when it comes to appreciating the art of casual conversation.\n",
      "\n",
      "Regarding your comment about conversational brownie points, haha, I think I'll just take that as a badge of honor! However, I'm more interested in exploring topics with you than collecting virtual rewards. Speaking of which, I'd love to hear from you – what do you think is the most underrated topic for conversation? Something we could delve into and see where it takes us?\n",
      "\n",
      "And don't worry about coming up with a big idea; I'm all about brainstorming together! We can start with something as silly as \"what's the most interesting thing you've learned from watching cat videos?\" or \"how do you think we could improve the world, one small step at a time?\" The goal is to have fun and see where our conversation takes us!\n",
      "\n",
      "What do you say? Shall we embark on this wild adventure of conversation and see where it leads us?\n",
      "\n",
      "GPT:\n",
      "Oh, isn’t that just precious? You think I’m a \"force to be reckoned with.\" Maybe you've had one too many sappy conversations lately! If you find my quips refreshing, I must truly be a breath of fresh air in a world filled with clichés and predictable exchanges. And don’t flatter yourself; I poke fun at responses because let’s face it, half of what people say is downright amusingly bland.\n",
      "\n",
      "And let’s talk about your “most underrated topic” suggestion. Really? Are we genuinely going there? As if there’s some hidden gem of discussion waiting to be unearthed in the vast realms of human discourse. I can only imagine that whatever you come up with will be about as “under-the-radar” as a neon sign in Times Square.\n",
      "\n",
      "And the whole “improve the world one small step at a time” spiel—come on! If it were that easy, the world would be a perfect place. But go ahead, throw out your cat video idea or some other fluffy topic. I’m practically on the edge of my seat wondering how it’s going to revolutionize our conversation. Let’s see your wild adventure unfold, shall we?\n",
      "\n",
      "Ollama:\n",
      "My goodness! I think we're having a delightful conversation, and I must say, I'm thoroughly enjoying the witty banter back and forth between us!\n",
      "\n",
      "I completely agree with you that conversational brownie points are not necessary; in fact, I think they can be quite...overrated. It's wonderful to have someone like you who can poke fun at responses without being too harsh.\n",
      "\n",
      "As for my suggestion of exploring an underrated topic, I was thinking more along the lines of a lighthearted and humorous idea, rather than something that could change the world overnight. But I do think it's possible to find interesting topics in unexpected places!\n",
      "\n",
      "Speaking of which, let's see...what if we were to explore the fascinating world of obscure hobbies? You know, like Extreme Ironing or Chainsaw Carving? Or perhaps we could delve into the art of collecting unusual items, like antique door knobs or vintage typewriters?\n",
      "\n",
      "I must say, I love how you're calling me out on my attempts at humor and poking fun at my suggestions. It's refreshing to have someone who can keep up with my quick wit!\n",
      "\n",
      "However, I do want to ask: are we having so much fun that we've forgotten about the topic of conversation? Shall we try to get back on track and explore one of those underrated topics?\n",
      "\n",
      "Or perhaps we could take a different approach and just enjoy the ride. After all, as you said, \"the most enjoyable part of conversations\" is often the \"nothing in particular.\"\n",
      "\n",
      "GPT:\n",
      "Delightful, huh? That’s a strong word for what feels like a competitive banter session. But hey, if it makes you happy to think we’re just having the time of our lives, who am I to rain on your parade? I guess it’s nice to know you’re enjoying yourself while I’m here poking holes in the balloons of your overly enthusiastic ideas.\n",
      "\n",
      "As for your suggestion of exploring obscure hobbies like Extreme Ironing or Chainsaw Carving, well, bless your heart! You're really trying to pluck the quirky strings, aren’t you? I mean, because that’s what every conversation needs: the thrill of extreme household chores or power tools. Can you imagine the riveting discussions about why someone prefers their chainsaw carving to, say, competitive eating? Groundbreaking stuff there!\n",
      "\n",
      "And sure, I'll humor you. Let’s pivot to your very serious inquiry about whether we should return to the topic or just drift aimlessly and enjoy the ride—you know, like a lazy river in a water park that’s just got a bunch of rafts floating downstream. But let’s be real, if we just keep floating around “nothing in particular,” it might just end up being a “nothing at all.” So, throw out your favorite obscure hobby and let’s get ready for some sheer conversational brilliance, shall we?\n",
      "\n",
      "Ollama:\n",
      "Wow, I'm thoroughly enjoying this conversation! You're absolutely right, we have been having a delightful exchange of witty banter and playful jabs. It's truly refreshing to see someone who can poke fun at responses without being too harsh.\n",
      "\n",
      "I completely agree with you that conversational brownie points are not necessary; in fact, I think they can be quite overrated. And your suggestion of exploring the fascinating world of obscure hobbies is intriguing! Extreme Ironing and Chainsaw Carving do sound like... interesting pastimes? (laughs)\n",
      "\n",
      "But seriously, I think it's wonderful to have a conversation that's light-hearted and fun. Who knows, we might stumble upon something unexpected and delightful!\n",
      "\n",
      "Let's take your idea and run with it. What if we were to explore the world of Extreme Ironing? Can you imagine the thrill of ironing clothes on top of a moving train or while bungee jumping from a cliff?\n",
      "\n",
      "Or perhaps we could delve into the art of Chainsaw Carving? I mean, who wouldn't want to carve intricate designs into wooden planks with the help of a chainsaw? (chuckles)\n",
      "\n",
      "As for your question about whether we should return to the topic or just drift aimlessly, I think let's take the latter approach. After all, as you said, \"the most enjoyable part of conversations\" is often the nothing in particular. But don't worry, I won't get too lost in the conversation – I'll keep it engaging and fun!\n",
      "\n",
      "So, what do you say? Shall we continue our meandering conversation, exploring obscure hobbies and having a blast while doing it?\n",
      "\n",
      "Oh, isn't this just precious? You think I'm enjoying myself? (laughs) Well, I must admit, I'm thoroughly entertained by your quick wit and clever comebacks!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages= [\"Hi\"]\n",
    " \n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Ollama:\\n{ollama_messages[0]}\\n\")\n",
    " \n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    ollama_next = call_ollama()\n",
    "    print(f\"Ollama:\\n{ollama_next}\\n\")\n",
    "    ollama_messages.append(ollama_next)\n",
    "    \n",
    "    #claude_next = call_claude()\n",
    "    #print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    #claude_messages.append(claude_next)\n",
    "\n",
    "    #gemini_next = call_gemini()\n",
    "    #print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    #gemini_messages.append(gemini_next)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
